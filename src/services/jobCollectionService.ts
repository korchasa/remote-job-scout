/**
 * Job Collection Service
 * –ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç —Å–±–æ—Ä –≤–∞–∫–∞–Ω—Å–∏–π –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
 */

import { randomUUID } from 'node:crypto';

import type { SearchRequest, Vacancy } from '../types/database.js';
import type { Country, JobPost, JobResponse, Scraper, ScraperInput } from '../types/scrapers.js';
import { countryFromString, Site } from '../types/scrapers.js';

export interface CollectionResult {
  success: boolean;
  vacancies: Vacancy[];
  totalCollected: number;
  sourcesProcessed: string[];
  errors: string[];
  sessionId: string;
}

export interface CollectionProgress {
  sessionId: string;
  currentSource: string;
  sourcesCompleted: number;
  totalSources: number;
  jobsCollected: number;
  errors: string[];
  isComplete: boolean;
}

export interface CollectionConfig {
  maxConcurrentSources: number; // –ú–∞–∫—Å–∏–º—É–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
  maxConcurrentPositions: number; // –ú–∞–∫—Å–∏–º—É–º –ø–æ–∑–∏—Ü–∏–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
  maxRetries: number; // –ú–∞–∫—Å–∏–º—É–º –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
  baseRetryDelay: number; // –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ backoff
  enableYamlSerialization: boolean; // –í–∫–ª—é—á–∏—Ç—å YAML —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—é
}

const defaultConfig: CollectionConfig = {
  maxConcurrentSources: 2,
  maxConcurrentPositions: 3,
  maxRetries: 3,
  baseRetryDelay: 1000, // 1 —Å–µ–∫—É–Ω–¥–∞
  enableYamlSerialization: true,
};

export class JobCollectionService {
  private activeSessions: Map<string, CollectionProgress> = new Map();

  constructor() {
    // JobCollectionService –±–æ–ª—å—à–µ –Ω–µ —Ö—Ä–∞–Ω–∏—Ç —Å–∫—Ä–µ–π–ø–µ—Ä—ã —Å–∞–º
  }

  /**
   * –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–±–æ—Ä–∞ –≤–∞–∫–∞–Ω—Å–∏–π
   */
  async collectJobs(scrapers: Scraper[], request: SearchRequest): Promise<CollectionResult> {
    const { session_id, settings } = request;
    const progress: CollectionProgress = {
      sessionId: session_id,
      currentSource: '',
      sourcesCompleted: 0,
      totalSources: 0,
      jobsCollected: 0,
      errors: [],
      isComplete: false,
    };

    this.activeSessions.set(session_id, progress);

    try {
      console.log(`üîç Starting job collection for session:`, {
        session: session_id,
        positions: settings.searchPositions,
        sources: settings.sources.jobSites,
      });

      // –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
      const sourcesToProcess = this.getSourcesToProcessWith(scrapers, settings);
      progress.totalSources = sourcesToProcess.length;

      const allVacancies: Vacancy[] = [];
      const processedSources: string[] = [];
      const errors: string[] = [];

      // –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏
      const sourceResults = await this.processSourcesInParallel(
        sourcesToProcess,
        settings,
        session_id,
        progress,
        scrapers,
      );

      // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
      for (const result of sourceResults) {
        if (result.success) {
          allVacancies.push(...result.vacancies);
          processedSources.push(result.source);
          progress.sourcesCompleted++;
          progress.jobsCollected = allVacancies.length;

          console.log(`‚úÖ ${result.source}: collected ${result.vacancies.length} jobs`);
        } else {
          errors.push(result.error!);
          progress.errors.push(result.error!);
          console.error(`‚ùå ${result.error}`);
        }
      }

      progress.isComplete = true;

      // –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≤ YAML –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
      if (defaultConfig.enableYamlSerialization && allVacancies.length > 0) {
        await this.serializeVacanciesToYaml(allVacancies, session_id);
      }

      const result: CollectionResult = {
        success: errors.length === 0,
        vacancies: allVacancies,
        totalCollected: allVacancies.length,
        sourcesProcessed: processedSources,
        errors,
        sessionId: session_id,
      };

      console.log(
        `üéâ Collection complete: ${allVacancies.length} jobs from ${processedSources.length} sources`,
      );

      return result;
    } catch (error) {
      progress.isComplete = true;
      progress.errors.push((error as Error).message);

      return {
        success: false,
        vacancies: [],
        totalCollected: 0,
        sourcesProcessed: [],
        errors: [(error as Error).message],
        sessionId: session_id,
      };
    }
  }

  /**
   * –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å —Å–±–æ—Ä–∞ –¥–ª—è —Å–µ—Å—Å–∏–∏
   */
  getProgress(sessionId: string): CollectionProgress | null {
    return this.activeSessions.get(sessionId) ?? null;
  }

  /**
   * –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–±–æ—Ä –¥–ª—è —Å–µ—Å—Å–∏–∏
   */
  stopCollection(sessionId: string): boolean {
    const progress = this.activeSessions.get(sessionId);
    if (progress && !progress.isComplete) {
      progress.isComplete = true;
      progress.errors.push('Collection stopped by user');
      return true;
    }
    return false;
  }

  /**
   * –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö —Å–∫—Ä–µ–π–ø–µ—Ä–æ–≤
   */
  private getSourcesToProcessWith(
    scrapers: Scraper[],
    settings: SearchRequest['settings'],
  ): string[] {
    const requestedSources = Object.keys(settings.sources).filter(
      (sourceName) => settings.sources[sourceName].enabled,
    );
    const available = new Set(scrapers.map((s) => s.getName().toLowerCase()));

    // –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    return requestedSources.filter((source) => available.has(source.toLowerCase()));
  }

  /**
   * –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫
   */
  private async processSource(
    source: string,
    settings: SearchRequest['settings'],
    sessionId: string,
    scrapers: Scraper[],
  ): Promise<Vacancy[]> {
    const scraper = scrapers.find((s) => s.getName().toLowerCase() === source.toLowerCase());
    if (!scraper) {
      throw new Error(`Scraper for ${source} not found`);
    }

    const allVacancies: Vacancy[] = [];

    // –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–∑–∏—Ü–∏–π
    const positionBatches = this.chunkArray(
      settings.searchPositions,
      defaultConfig.maxConcurrentPositions,
    );

    for (const batch of positionBatches) {
      const positionPromises = batch.map((position) =>
        this.processPosition(scraper, source, position, settings, sessionId),
      );

      const batchResults = await Promise.all(positionPromises);
      for (const vacancies of batchResults) {
        allVacancies.push(...vacancies);
      }
    }

    return allVacancies;
  }

  /**
   * –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–Ω—É –ø–æ–∑–∏—Ü–∏—é –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
   */
  private async processPosition(
    scraper: Scraper,
    source: string,
    position: string,
    settings: SearchRequest['settings'],
    sessionId: string,
  ): Promise<Vacancy[]> {
    const vacancies: Vacancy[] = [];

    let country: Country | undefined;
    try {
      if (
        settings.filters?.countries &&
        Array.isArray(settings.filters.countries) &&
        settings.filters.countries.length > 0
      ) {
        country = countryFromString(settings.filters.countries[0]);
      }
    } catch (error) {
      console.warn(`‚ö†Ô∏è Failed to parse country "${settings.filters.countries[0]}":`, error);
      // Continue without country filter
    }

    // –û–ø—Ä–µ–¥–µ–ª—è–µ–º site_type –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    let siteType: Site;
    switch (source.toLowerCase()) {
      case 'indeed':
        siteType = Site.INDEED;
        break;
      case 'linkedin':
        siteType = Site.LINKEDIN;
        break;
      case 'openai':
        siteType = Site.OPENAI;
        break;
      case 'google':
        siteType = Site.GOOGLE;
        break;
      default:
        siteType = Site.INDEED; // fallback
    }

    const input: ScraperInput = {
      site_type: [siteType],
      search_term: position,
      location:
        settings.filters?.countries &&
        Array.isArray(settings.filters.countries) &&
        settings.filters.countries.length > 0
          ? settings.filters.countries[0]
          : undefined,
      country: country,
      is_remote: true, // –§–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ remote –≤–∞–∫–∞–Ω—Å–∏—è—Ö
      results_wanted: 25, // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    };

    const response: JobResponse = await scraper.scrape(input);

    if (response.jobs.length === 0) {
      console.warn(`‚ö†Ô∏è ${source} no jobs found for ${position}`);
    }

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ response.jobs —è–≤–ª—è–µ—Ç—Å—è –º–∞—Å—Å–∏–≤–æ–º
    if (!response.jobs || !Array.isArray(response.jobs)) {
      console.error(`‚ùå ${source} returned invalid jobs data for ${position}:`, response.jobs);
      return vacancies;
    }

    // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º JobPost –≤ Vacancy
    for (const job of response.jobs) {
      if (job && typeof job === 'object') {
        const vacancy = this.convertJobToVacancy(job, sessionId, source);
        vacancies.push(vacancy);
      } else {
        console.warn(`‚ö†Ô∏è ${source} returned invalid job object for ${position}:`, job);
      }
    }

    return vacancies;
  }

  /**
   * –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å JobPost –≤ Vacancy
   */
  private convertJobToVacancy(job: JobPost, sessionId: string, source?: string): Vacancy {
    return {
      id: randomUUID(),
      title: job.title,
      description: job.description ?? '',
      url: job.job_url,
      published_date: job.date_posted ? job.date_posted.toISOString() : undefined,
      status: 'collected',
      created_at: new Date().toISOString(),
      collected_at: new Date().toISOString(),
      source: source ?? 'unknown',
      country: job.location?.country ?? undefined,
      session_id: sessionId,
      // data –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ
      data: JSON.stringify({
        company: job.company_name,
        location: job.location,
        is_remote: job.is_remote,
        job_type: job.job_type,
        compensation: job.compensation,
        emails: job.emails,
      }),
    };
  }

  /**
   * –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏–∏
   */
  private async processSourcesInParallel(
    sources: string[],
    settings: SearchRequest['settings'],
    sessionId: string,
    progress: CollectionProgress,
    scrapers: Scraper[],
  ): Promise<Array<{ source: string; success: boolean; vacancies: Vacancy[]; error?: string }>> {
    const results: Array<{
      source: string;
      success: boolean;
      vacancies: Vacancy[];
      error?: string;
    }> = [];

    // –†–∞–∑–±–∏–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    const batches = this.chunkArray(sources, defaultConfig.maxConcurrentSources);

    for (const batch of batches) {
      const batchPromises = batch.map((source) =>
        this.processSourceWithRetry(source, settings, sessionId, progress, scrapers),
      );

      const batchResults = await Promise.all(batchPromises);
      results.push(...batchResults);
    }

    return results;
  }

  /**
   * –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å retry
   */
  private async processSourceWithRetry(
    source: string,
    settings: SearchRequest['settings'],
    sessionId: string,
    progress: CollectionProgress,
    scrapers: Scraper[],
  ): Promise<{ source: string; success: boolean; vacancies: Vacancy[]; error?: string }> {
    let lastError: Error | null = null;

    for (let attempt = 0; attempt <= defaultConfig.maxRetries; attempt++) {
      try {
        progress.currentSource = source;
        const vacancies = await this.processSource(source, settings, sessionId, scrapers);
        return { source, success: true, vacancies };
      } catch (error) {
        lastError = error as Error;

        if (attempt < defaultConfig.maxRetries) {
          const delay = defaultConfig.baseRetryDelay * Math.pow(2, attempt);
          console.log(
            `‚è≥ Retrying ${source} in ${delay}ms (attempt ${attempt + 1}/${
              defaultConfig.maxRetries + 1
            })`,
          );
          await new Promise((resolve) => setTimeout(resolve, delay));
        }
      }
    }

    return {
      source,
      success: false,
      vacancies: [],
      error: `${source}: ${lastError?.message ?? 'Unknown error'}`,
    };
  }

  /**
   * –†–∞–∑–±–∏—Ç—å –º–∞—Å—Å–∏–≤ –Ω–∞ —á–∞–Ω–∫–∏
   */
  private chunkArray<T>(array: T[], size: number): T[][] {
    const result: T[][] = [];
    for (let i = 0; i < array.length; i += size) {
      result.push(array.slice(i, i + size));
    }
    return result;
  }

  /**
   * –°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∫–∞–Ω—Å–∏–π –≤ YAML
   */
  private async serializeVacanciesToYaml(vacancies: Vacancy[], sessionId: string): Promise<void> {
    try {
      const { writeFileSync, mkdirSync } = await import('node:fs');
      const { dirname, resolve } = await import('node:path');
      const yamlModule = await import('js-yaml');
      const yaml = yamlModule.default;

      const filePath = resolve('data', 'jobs', `${sessionId}.yml`);
      mkdirSync(dirname(filePath), { recursive: true });

      const data = {
        session_id: sessionId,
        total: vacancies.length,
        vacancies: vacancies.map((v) => ({
          id: v.id,
          title: v.title,
          company: JSON.parse(v.data ?? '{}').company,
          url: v.url,
          published_date: v.published_date ?? null,
          source: v.source ?? 'unknown',
        })),
      };

      writeFileSync(filePath, yaml.dump(data), 'utf8');
    } catch (error) {
      console.error('‚ùå Failed to serialize vacancies to YAML:', error);
    }
  }
}
